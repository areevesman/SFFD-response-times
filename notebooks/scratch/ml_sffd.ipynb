{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "import boto3\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "ss = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'msds697jonross.and.friends' # Add your bucket name\n",
    "file_name = 'sffd.csv' # select file\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(bucket_name) \n",
    "obj = bucket.Object(key=file_name) # S3 uses key-value structure where key is your file name\n",
    "file_content = obj.get()[\"Body\"].read().decode(\"utf-8\") # Read the Body which is the contents of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4557045"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of rows (subract header and empty line at end)\n",
    "rows = file_content.split('\\n')\n",
    "len(rows)-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of  columns\n",
    "column_names = rows[0].split(',')\n",
    "n_cols = sc.broadcast(len(column_names))\n",
    "n_cols.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call_type\n",
      "received_timestamp\n",
      "entry_timestamp\n",
      "dispatch_timestamp\n",
      "response_timestamp\n",
      "on_scene_timestamp\n",
      "transport_timestamp\n",
      "hospital_timestamp\n",
      "call_final_disposition\n",
      "available_timestamp\n",
      "address\n",
      "zipcode_of_incident\n",
      "battalion\n",
      "station_area\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(x for x in column_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Alarms,2004-10-24 11:16:45+00:00,2004-10-24 11:18:15+00:00,2004-10-24 11:18:23+00:00,2004-10-24 11:18:31+00:00,2004-10-24 11:21:52+00:00,,,Other,2004-10-24 11:24:41+00:00,0 Block of SAN FERNANDO WAY,94127,B08,A3',\n",
       "       'Medical Incident,2012-02-18 01:36:32+00:00,2012-02-18 01:39:11+00:00,2012-02-18 01:39:48+00:00,2012-02-18 01:39:57+00:00,2012-02-18 01:50:19+00:00,2012-02-18 02:02:45+00:00,2012-02-18 02:08:54+00:00,Other,2012-02-18 02:31:20+00:00,3100 Block of FILLMORE ST,94123,B04,16'],\n",
       "      dtype='<U315')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly sample rows\n",
    "sz=100000\n",
    "samples = np.random.choice(rows[1:], size=sz, replace=False)\n",
    "samples[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fire(x):\n",
    "    return len(x.split(',')) == n_cols.value\n",
    "\n",
    "rdd = sc.parallelize(list(samples))\\\n",
    "    .filter(filter_fire)\\\n",
    "    .map(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of rows removed\n",
    "sz - rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"call_type\", StringType(), False),\n",
    "                    StructField(\"received_timestamp\", StringType(), False),\n",
    "                    StructField(\"entry_timestamp\", StringType(), False),\n",
    "                    StructField(\"dispatch_timestamp\", StringType(), False),\n",
    "                    StructField(\"response_timestamp\", StringType(), False),\n",
    "                    StructField(\"on_scene_timestamp\", StringType(), False),\n",
    "                    StructField(\"transport_timestamp\", StringType(), False),\n",
    "                    StructField(\"hospital_timestamp\", StringType(), False),\n",
    "                    StructField(\"call_final_disposition\", StringType(), False),\n",
    "                    StructField(\"available_timestamp\", StringType(), False),\n",
    "                    StructField(\"address\", StringType(), False),\n",
    "                    StructField(\"zipcode_of_incident\", StringType(), False),\n",
    "                    StructField(\"battalion\", StringType(), False),\n",
    "                    StructField(\"station_area\", StringType(), False)\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ss.createDataFrame(rdd, schema) # .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- call_type: string (nullable = false)\n",
      " |-- received_timestamp: string (nullable = false)\n",
      " |-- entry_timestamp: string (nullable = false)\n",
      " |-- dispatch_timestamp: string (nullable = false)\n",
      " |-- response_timestamp: string (nullable = false)\n",
      " |-- on_scene_timestamp: string (nullable = false)\n",
      " |-- transport_timestamp: string (nullable = false)\n",
      " |-- hospital_timestamp: string (nullable = false)\n",
      " |-- call_final_disposition: string (nullable = false)\n",
      " |-- available_timestamp: string (nullable = false)\n",
      " |-- address: string (nullable = false)\n",
      " |-- zipcode_of_incident: string (nullable = false)\n",
      " |-- battalion: string (nullable = false)\n",
      " |-- station_area: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to timestamps\n",
    "my_rows = ['received_timestamp',\n",
    "          'entry_timestamp',\n",
    "          'dispatch_timestamp',\n",
    "          'response_timestamp',\n",
    "          'on_scene_timestamp',\n",
    "          'transport_timestamp',\n",
    "          'hospital_timestamp',\n",
    "          'available_timestamp']\n",
    "\n",
    "df_w_time = df\n",
    "for row in my_rows:\n",
    "    df_w_time = df_w_time.withColumn(row, to_timestamp(df[row], format = 'yyyy-MM-dd HH:mm:ss+00:00'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "small_df = df_w_time.select('call_type',\n",
    "                 'received_timestamp',\n",
    "                 'on_scene_timestamp',\n",
    "                 'zipcode_of_incident',\n",
    "                 'battalion',\n",
    "                 'station_area')\\\n",
    "    .withColumn(\"label\", \n",
    "                (unix_timestamp('on_scene_timestamp') - unix_timestamp('received_timestamp')) / 60)\\\n",
    "    .orderBy('received_timestamp', ascending=[0])\\\n",
    "    .na.drop(subset=[\"label\"])\\\n",
    "    .select('call_type',\n",
    "            'zipcode_of_incident',\n",
    "            'battalion',\n",
    "            'station_area',\n",
    "            'label')\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+---------+------------+-----------------+\n",
      "|       call_type|zipcode_of_incident|battalion|station_area|            label|\n",
      "+----------------+-------------------+---------+------------+-----------------+\n",
      "|Medical Incident|              94131|      B06|          11|5.616666666666666|\n",
      "|Medical Incident|              94110|      B06|          09|              4.1|\n",
      "|Medical Incident|              94110|      B06|          07|5.233333333333333|\n",
      "|Medical Incident|              94108|      B01|          13|              5.6|\n",
      "|Medical Incident|              94111|      B01|          13|8.883333333333333|\n",
      "+----------------+-------------------+---------+------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "76624\n"
     ]
    }
   ],
   "source": [
    "small_df.show(5)\n",
    "print(small_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_strings(df):\n",
    "    return df.replace('','unknown')\n",
    "dfnonas = remove_empty_strings(small_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting strings to numeric values\n",
    "def indexStringColumns(df, cols):\n",
    "    #variable newdf will be updated several times\n",
    "    newdf = df\n",
    "    \n",
    "    for c in cols:\n",
    "        #For each given colum, fits StringIndexerModel.\n",
    "        si = StringIndexer(inputCol=c, outputCol=c+\"-num\")\n",
    "        sm = si.fit(newdf)\n",
    "        #Creates a DataFame by putting the transformed values in the new colum with suffix \"-num\" \n",
    "        #and then drops the original columns.\n",
    "        #and drop the \"-num\" suffix. \n",
    "        newdf = sm.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-num\", c)\n",
    "    return newdf\n",
    "\n",
    "dfnumeric = indexStringColumns(dfnonas, [\"call_type\", \"zipcode_of_incident\", \"battalion\", \"station_area\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+-------------------+---------+------------+\n",
      "|             label|call_type|zipcode_of_incident|battalion|station_area|\n",
      "+------------------+---------+-------------------+---------+------------+\n",
      "| 5.616666666666666|      0.0|               20.0|      7.0|        11.0|\n",
      "|               4.1|      0.0|                2.0|      7.0|        36.0|\n",
      "| 5.233333333333333|      0.0|                2.0|      7.0|         3.0|\n",
      "|               5.6|      0.0|               17.0|      2.0|         5.0|\n",
      "| 8.883333333333333|      0.0|               21.0|      2.0|         5.0|\n",
      "|              5.25|      3.0|                1.0|      1.0|        28.0|\n",
      "| 7.166666666666667|      2.0|               18.0|      0.0|        24.0|\n",
      "| 6.916666666666667|      0.0|               20.0|      7.0|        38.0|\n",
      "| 6.683333333333334|      1.0|               10.0|      8.0|         4.0|\n",
      "| 9.033333333333333|      0.0|                3.0|      3.0|         1.0|\n",
      "|               0.0|      5.0|                3.0|      3.0|        16.0|\n",
      "|3.0833333333333335|      1.0|                0.0|      1.0|         1.0|\n",
      "|              5.75|      0.0|               10.0|      8.0|        13.0|\n",
      "| 7.483333333333333|      0.0|                3.0|      3.0|         1.0|\n",
      "| 8.383333333333333|      0.0|                0.0|      1.0|         1.0|\n",
      "| 7.366666666666666|      0.0|                3.0|      3.0|        16.0|\n",
      "| 6.633333333333334|      0.0|               11.0|      9.0|        15.0|\n",
      "|             11.85|      0.0|                1.0|      0.0|         0.0|\n",
      "| 7.183333333333334|      0.0|                3.0|      3.0|         1.0|\n",
      "|13.283333333333333|      0.0|                1.0|      1.0|         4.0|\n",
      "+------------------+---------+-------------------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfnumeric.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncodeColumns(df, cols):\n",
    "    newdf = df\n",
    "    for c in cols:\n",
    "        #For each given colum, create OneHotEncoder. \n",
    "        #dropLast : Whether to drop the last category in the encoded vector (default: true)\n",
    "        onehotenc = OneHotEncoder(inputCol=c, outputCol=c+\"-onehot\", dropLast=False)\n",
    "        #Creates a DataFame by putting the transformed values in the new colum with suffix \"-onehot\" \n",
    "        #and then drops the original columns.\n",
    "        #and drop the \"-onehot\" suffix. \n",
    "        newdf = onehotenc.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-onehot\", c)\n",
    "    return newdf\n",
    "\n",
    "dfhot = oneHotEncodeColumns(dfnumeric, [\"call_type\", \"zipcode_of_incident\", \"battalion\", \"station_area\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+-------------------+--------------+---------------+\n",
      "|             label|     call_type|zipcode_of_incident|     battalion|   station_area|\n",
      "+------------------+--------------+-------------------+--------------+---------------+\n",
      "| 5.616666666666666|(29,[0],[1.0])|    (28,[20],[1.0])|(11,[7],[1.0])|(51,[11],[1.0])|\n",
      "|               4.1|(29,[0],[1.0])|     (28,[2],[1.0])|(11,[7],[1.0])|(51,[36],[1.0])|\n",
      "| 5.233333333333333|(29,[0],[1.0])|     (28,[2],[1.0])|(11,[7],[1.0])| (51,[3],[1.0])|\n",
      "|               5.6|(29,[0],[1.0])|    (28,[17],[1.0])|(11,[2],[1.0])| (51,[5],[1.0])|\n",
      "| 8.883333333333333|(29,[0],[1.0])|    (28,[21],[1.0])|(11,[2],[1.0])| (51,[5],[1.0])|\n",
      "|              5.25|(29,[3],[1.0])|     (28,[1],[1.0])|(11,[1],[1.0])|(51,[28],[1.0])|\n",
      "| 7.166666666666667|(29,[2],[1.0])|    (28,[18],[1.0])|(11,[0],[1.0])|(51,[24],[1.0])|\n",
      "| 6.916666666666667|(29,[0],[1.0])|    (28,[20],[1.0])|(11,[7],[1.0])|(51,[38],[1.0])|\n",
      "| 6.683333333333334|(29,[1],[1.0])|    (28,[10],[1.0])|(11,[8],[1.0])| (51,[4],[1.0])|\n",
      "| 9.033333333333333|(29,[0],[1.0])|     (28,[3],[1.0])|(11,[3],[1.0])| (51,[1],[1.0])|\n",
      "|               0.0|(29,[5],[1.0])|     (28,[3],[1.0])|(11,[3],[1.0])|(51,[16],[1.0])|\n",
      "|3.0833333333333335|(29,[1],[1.0])|     (28,[0],[1.0])|(11,[1],[1.0])| (51,[1],[1.0])|\n",
      "|              5.75|(29,[0],[1.0])|    (28,[10],[1.0])|(11,[8],[1.0])|(51,[13],[1.0])|\n",
      "| 7.483333333333333|(29,[0],[1.0])|     (28,[3],[1.0])|(11,[3],[1.0])| (51,[1],[1.0])|\n",
      "| 8.383333333333333|(29,[0],[1.0])|     (28,[0],[1.0])|(11,[1],[1.0])| (51,[1],[1.0])|\n",
      "| 7.366666666666666|(29,[0],[1.0])|     (28,[3],[1.0])|(11,[3],[1.0])|(51,[16],[1.0])|\n",
      "| 6.633333333333334|(29,[0],[1.0])|    (28,[11],[1.0])|(11,[9],[1.0])|(51,[15],[1.0])|\n",
      "|             11.85|(29,[0],[1.0])|     (28,[1],[1.0])|(11,[0],[1.0])| (51,[0],[1.0])|\n",
      "| 7.183333333333334|(29,[0],[1.0])|     (28,[3],[1.0])|(11,[3],[1.0])| (51,[1],[1.0])|\n",
      "|13.283333333333333|(29,[0],[1.0])|     (28,[1],[1.0])|(11,[1],[1.0])| (51,[4],[1.0])|\n",
      "+------------------+--------------+-------------------+--------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfhot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Training and Test data.\n",
    "sets = dfhot.randomSplit([0.8, 0.2])\n",
    "train = sets[0] # .cache()\n",
    "test = sets[1] # .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['call_type', 'zipcode_of_incident', 'battalion', 'station_area']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_cols = small_df.columns[:-1]\n",
    "in_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "va = VectorAssembler(outputCol=\"features\",\n",
    "                     inputCols=in_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_for_model = va.transform(train)\\\n",
    "    .select(\"features\", \"label\")\n",
    "test_for_model = va.transform(test)\\\n",
    "    .select(\"features\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+-----+\n",
      "|features                            |label|\n",
      "+------------------------------------+-----+\n",
      "|(119,[0,30,57,68],[1.0,1.0,1.0,1.0])|0.0  |\n",
      "+------------------------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_for_model.show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit Random Forest Regressor\n",
    "rf = RandomForestRegressor()\n",
    "rf_fitted = rf.fit(train_for_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluate model results\n",
    "test_pred = rf_fitted.transform(test_for_model)\n",
    "r2_ev = RegressionEvaluator(metricName='r2')\n",
    "rmse_ev = RegressionEvaluator(metricName='rmse')\n",
    "mae_ev = RegressionEvaluator(metricName='mae')\n",
    "r2 = r2_ev.evaluate(test_pred)\n",
    "rmse = rmse_ev.evaluate(test_pred)\n",
    "mae = mae_ev.evaluate(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:  0.0038455312058562896\n",
      "RMSE: 9.660442607036526\n",
      "MAE:  4.3166788231828415\n"
     ]
    }
   ],
   "source": [
    "print(f\"R^2:  {r2}\\nRMSE: {rmse}\\nMAE:  {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
