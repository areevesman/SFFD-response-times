{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import Pipeline\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "ss = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4557046"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_fire = sc.textFile(\"s3a://msds697jonross.and.friends/sffd.csv\")\\\n",
    "             .map(lambda line : line.encode('ascii', 'ignore'))\n",
    "rdd_fire.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32074"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_google = sc.textFile(\"s3a://msds697jonross.and.friends/google_data.csv\")\\\n",
    "               .map(lambda line : line.encode('ascii', 'ignore'))\n",
    "rdd_google.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of  columns\n",
    "cols_fire = rdd_fire.map(lambda x: x.split(',')).take(1)[0]\n",
    "n_cols = sc.broadcast(len(cols_fire))\n",
    "n_cols.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call_type\n",
      "received_timestamp\n",
      "entry_timestamp\n",
      "dispatch_timestamp\n",
      "response_timestamp\n",
      "on_scene_timestamp\n",
      "transport_timestamp\n",
      "hospital_timestamp\n",
      "call_final_disposition\n",
      "available_timestamp\n",
      "address\n",
      "zipcode_of_incident\n",
      "battalion\n",
      "station_area\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(x for x in cols_fire))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_list = [i for i in range(1,52)]\n",
    "for num in [27,30,45,46,47,50]:\n",
    "    station_list.remove(num)\n",
    "station_list = [str(a) for a in station_list]\n",
    "station_list = [\"0\" + a if len(a)==1 else a for a in station_list]\n",
    "stations = sc.broadcast(station_list)\n",
    "# stations.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fire(x):\n",
    "    if len(x.split(',')) == n_cols.value:\n",
    "        return x.split(',')[13] in stations.value\n",
    "\n",
    "def map_fire(x):\n",
    "    return ((x.split(',')[13],x.split(',')[10]), x.split(',')[:2]+[x.split(',')[5]]+[x.split(',')[11]])\n",
    "\n",
    "def map_google(x):\n",
    "    s = x.split(',')\n",
    "    try:\n",
    "        if int(s[0]) < 10:\n",
    "            s[0] = '0' + s[0]\n",
    "    except:\n",
    "        pass\n",
    "    return (tuple(s[:2]), s[2:])\n",
    "\n",
    "def map_joined(x):\n",
    "    try:\n",
    "        return list(x[0])+x[1][0]+[float(y) for y in x[1][1]]\n",
    "    except:\n",
    "        return list(x[0])+x[1][0]+[None,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_fire_data = rdd_fire.filter(lambda x: 'call_type' not in x)\\\n",
    "                        .filter(filter_fire)\\\n",
    "                        .map(map_fire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('04', '2700 Block of VAN NESS AVE'),\n",
       "  ['Elevator / Escalator Rescue', '2000-06-03 15:32:02+00:00', '', '94123']),\n",
       " (('20', '200 Block of PALO ALTO AVE'),\n",
       "  ['Alarms', '2009-10-14 16:11:41+00:00', '', '94114'])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_fire_data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('01', '0 Block of 0NB OCTAVIA OF'), ['3724', '12.883333333333333']),\n",
       " (('01', '0 Block of 101 NB OCTAVIA OF'), ['4290', '9.416666666666666'])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_google = rdd_google.map(map_google)\n",
    "rdd_google.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = rdd_fire_data.leftOuterJoin(rdd_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('11', 'BERNAL HEIGHTS BL/ANDERSON ST'),\n",
       "  (['Medical Incident',\n",
       "    '2008-03-27 18:09:32+00:00',\n",
       "    '2008-03-27 18:20:21+00:00',\n",
       "    '94110'],\n",
       "   ['2032', '7.6'])),\n",
       " (('11', 'BERNAL HEIGHTS BL/ANDERSON ST'),\n",
       "  (['Medical Incident',\n",
       "    '2008-03-27 18:09:32+00:00',\n",
       "    '2008-03-27 18:11:46+00:00',\n",
       "    '94110'],\n",
       "   ['2032', '7.6']))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = joined.map(map_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['11',\n",
       "  'BERNAL HEIGHTS BL/ANDERSON ST',\n",
       "  'Medical Incident',\n",
       "  '2008-03-27 18:09:32+00:00',\n",
       "  '2008-03-27 18:20:21+00:00',\n",
       "  '94110',\n",
       "  2032.0,\n",
       "  7.6],\n",
       " ['11',\n",
       "  'BERNAL HEIGHTS BL/ANDERSON ST',\n",
       "  'Medical Incident',\n",
       "  '2008-03-27 18:09:32+00:00',\n",
       "  '2008-03-27 18:11:46+00:00',\n",
       "  '94110',\n",
       "  2032.0,\n",
       "  7.6]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"station_area\", StringType(), False),\n",
    "                     StructField(\"address\", StringType(), False),\n",
    "                     StructField(\"call_type\", StringType(), False),\n",
    "                     StructField(\"received_timestamp\", StringType(), False),\n",
    "                     StructField(\"on_scene_timestamp\", StringType(), False),\n",
    "                     StructField(\"zipcode_of_incident\", StringType(), False),\n",
    "                     StructField(\"distance\", DoubleType(), True),\n",
    "                     StructField(\"duration\", DoubleType(), True)\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ss.createDataFrame(rdd, schema) # .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_area: string (nullable = false)\n",
      " |-- address: string (nullable = false)\n",
      " |-- call_type: string (nullable = false)\n",
      " |-- received_timestamp: string (nullable = false)\n",
      " |-- on_scene_timestamp: string (nullable = false)\n",
      " |-- zipcode_of_incident: string (nullable = false)\n",
      " |-- distance: double (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to timestamps\n",
    "my_rows = ['received_timestamp',\n",
    "          'on_scene_timestamp']\n",
    "\n",
    "df_w_time = df\n",
    "for row in my_rows:\n",
    "    df_w_time = df_w_time.withColumn(row, to_timestamp(df[row], format = 'yyyy-MM-dd HH:mm:ss+00:00'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_time = df_w_time.withColumn(\"received_hour\",\n",
    "                                 hour(\"received_timestamp\")) \\\n",
    "            .withColumn(\"day_of_week\",\n",
    "                        date_format('received_timestamp', 'u').alias('dow_number'))\n",
    "# df_w_time.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create More Interesting DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = df_w_time.select('station_area',\n",
    "                 'address',\n",
    "                 'call_type',\n",
    "                 'received_timestamp',\n",
    "                 'on_scene_timestamp',\n",
    "                 'zipcode_of_incident',\n",
    "                 'distance',\n",
    "                 'duration',\n",
    "                 'received_hour',\n",
    "                 'day_of_week')\\\n",
    "    .withColumn(\"distance\", \n",
    "                col('distance') / 1609.34 )\\\n",
    "    .withColumn(\"label\", \n",
    "                (unix_timestamp('on_scene_timestamp') - unix_timestamp('received_timestamp')) / 60 )\\\n",
    "    .withColumn(\"speed\", \n",
    "                col('distance') / col('duration') * 60 )\\\n",
    "    .withColumn(\"label_ratio\", \n",
    "                col('label') / col('duration') )\\\n",
    "    .orderBy('received_timestamp', ascending=[0])\\\n",
    "    .na.drop(subset=[\"label\"])\\\n",
    "    .where('label >= 0')\\\n",
    "    .select('station_area',\n",
    "        'call_type',\n",
    "        'zipcode_of_incident',\n",
    "        'distance',\n",
    "        'duration',\n",
    "        'speed',\n",
    "        'label_ratio',\n",
    "        'received_hour',\n",
    "        'day_of_week',\n",
    "        'label')\n",
    "\n",
    "# data_1 is the largest dataset\n",
    "# has all columns and ony removes missing google data\n",
    "data_1 = data_full.na.drop(subset=[\"distance\", \"duration\"]) # (removes about 50 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_area: string (nullable = false)\n",
      " |-- call_type: string (nullable = false)\n",
      " |-- zipcode_of_incident: string (nullable = false)\n",
      " |-- distance: double (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- speed: double (nullable = true)\n",
      " |-- label_ratio: double (nullable = true)\n",
      " |-- received_hour: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `label`: response time\n",
    "- `duration`: google api estimate of driving time from station to address (in minutes)\n",
    "- `distance`: google api estimate of driving distance from station to address\n",
    "- `speed`: distance / duration * 60 (estimated average speed in mph)\n",
    "- `label_ratio`: label / duration. **Using this variable for eda**. The idea is to try to find the observations where distance and duration would underestimate most severely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_1.approxQuantile(\"distance\", [.0001,.001,.5,.9,.97,.98,.99,.999], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing rows where distance (in miles) is too large or too small\n",
    "data_2 = data_1.where('distance between .01 and 2') \n",
    "#print('removed' + (data_1.count() - data_2.count()) + 'rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see that speeds look reasonable (mph)\n",
    "#data_2.approxQuantile(\"speed\", [.001,.01,.02,.03,.1,.5,.9,.96,.97,.98,.99,.999], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_2.approxQuantile(\"label\", [.001,.01,.02,.03,.1,.5,.9,.96,.97,.98,.99,.999], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing rows where response time (in minutes) is too large or too small\n",
    "data_3 = data_2.where('label between 1 and 30') \n",
    "#print('removed' + (data_2.count() - data_3.count() + 'rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_3.approxQuantile(\"label_ratio\", [.001,.01,.1,.5,.9,.96,.97,.98,.99,.999], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing rows where response time is too much larger than duration\n",
    "data_4 = data_3.where('label_ratio < 20') \n",
    "#print('removed' + (data_3.count() - data_4.count()) + 'rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = data_4\\\n",
    "    .select('station_area','call_type','zipcode_of_incident',\n",
    "            'distance','duration','received_hour','day_of_week','label')\n",
    "#data_cleaned.select('station_area','call_type','zipcode_of_incident').show()\n",
    "#data_cleaned.select('distance','duration','received_hour','day_of_week','label').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_strings(df):\n",
    "    return df.replace('','unknown')\n",
    "# dfnonas = remove_empty_strings(small_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting strings to numeric values\n",
    "def indexStringColumns(df, cols):\n",
    "    #variable newdf will be updated several times\n",
    "    newdf = df\n",
    "    \n",
    "    for c in cols:\n",
    "        #For each given colum, fits StringIndexerModel.\n",
    "        si = StringIndexer(inputCol=c, outputCol=c+\"-num\")\n",
    "        sm = si.fit(newdf)\n",
    "        #Creates a DataFame by putting the transformed values in the new colum with suffix \"-num\" \n",
    "        #and then drops the original columns.\n",
    "        #and drop the \"-num\" suffix. \n",
    "        newdf = sm.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-num\", c)\n",
    "    return newdf\n",
    "\n",
    "# dfnumeric = indexStringColumns(dfnonas, [\"call_type\", \"zipcode_of_incident\", \"station_area\"])\n",
    "# dfnumeric = indexStringColumns(dfnonas, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncodeColumns(df, cols):\n",
    "    newdf = df\n",
    "    for c in cols:\n",
    "        #For each given colum, create OneHotEncoder. \n",
    "        #dropLast : Whether to drop the last category in the encoded vector (default: true)\n",
    "        onehotenc = OneHotEncoder(inputCol=c, outputCol=c+\"-onehot\", dropLast=False)\n",
    "        #Creates a DataFame by putting the transformed values in the new colum with suffix \"-onehot\" \n",
    "        #and then drops the original columns.\n",
    "        #and drop the \"-onehot\" suffix. \n",
    "        newdf = onehotenc.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-onehot\", c)\n",
    "    return newdf\n",
    "\n",
    "# dfhot = oneHotEncodeColumns(dfnumeric, [\"call_type\", \"zipcode_of_incident\", \"station_area\"])\n",
    "# dfhot = oneHotEncodeColumns(dfnumeric, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df, cat_cols, hot_cols):\n",
    "    dfnonas = remove_empty_strings(df)\n",
    "    dfnumeric = indexStringColumns(dfnonas, cat_cols)\n",
    "    dfhot = oneHotEncodeColumns(dfnumeric, hot_cols)\n",
    "    return dfhot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+\n",
      "|        distance_log|      duration_log|             label|\n",
      "+--------------------+------------------+------------------+\n",
      "| -0.5800741784344655|1.3652409519220583|              8.35|\n",
      "|  -1.041458017321652| 0.597003320007043|               5.8|\n",
      "|  -1.041458017321652| 0.597003320007043|             10.25|\n",
      "| -0.8367940252822796|0.8685000680378065|11.966666666666667|\n",
      "| -0.3483108367617068|1.3987168811184478|               3.5|\n",
      "| -0.3483108367617068|1.3987168811184478|               8.0|\n",
      "| -0.3483108367617068|1.3987168811184478| 6.716666666666667|\n",
      "| -0.3483108367617068|1.3987168811184478|3.0166666666666666|\n",
      "| -0.3483108367617068|1.3987168811184478|              4.45|\n",
      "| -0.3483108367617068|1.3987168811184478| 4.783333333333333|\n",
      "| -0.3483108367617068|1.3987168811184478|              4.85|\n",
      "| -0.3483108367617068|1.3987168811184478|3.0166666666666666|\n",
      "| -0.3509551750147972|1.3217558399823195|             19.95|\n",
      "| -0.7528960504004318|   1.2947271675944|               7.5|\n",
      "| -0.6853113819273905|0.9994056385846617|23.216666666666665|\n",
      "| -0.6853113819273905|0.9994056385846617| 7.116666666666666|\n",
      "|-0.44045701322337505|1.4271163556401458|10.383333333333333|\n",
      "| -0.3308583868104807|1.3437347467010947| 8.433333333333334|\n",
      "| -0.3308583868104807|1.3437347467010947| 5.616666666666666|\n",
      "|0.046534702518997224|1.7047480922384253|               4.1|\n",
      "+--------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_cleaned.select('distance','duration','label')\\\n",
    "    .withColumn('distance_log', log(col('distance')))\\\n",
    "    .withColumn('duration_log', log(col('duration')))\\\n",
    "    .select('distance_log','duration_log','label')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_small = data_cleaned.select('distance','duration','label')\\\n",
    "    .withColumn('distance_log', log(col('distance')))\\\n",
    "    .withColumn('duration_log', log(col('duration')))\\\n",
    "    .select('distance_log','duration_log','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_big = data_cleaned.withColumn('distance_log', log(col('distance')))\\\n",
    "    .withColumn('duration_log', log(col('duration')))\\\n",
    "    .drop('distance','duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+\n",
      "|       distance_log|      duration_log|             label|\n",
      "+-------------------+------------------+------------------+\n",
      "|-0.5800741784344655|1.3652409519220583|              8.35|\n",
      "| -1.041458017321652| 0.597003320007043|             10.25|\n",
      "| -1.041458017321652| 0.597003320007043|               5.8|\n",
      "|-0.8367940252822796|0.8685000680378065|11.966666666666667|\n",
      "|-0.3483108367617068|1.3987168811184478|3.0166666666666666|\n",
      "+-------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_small.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+-------------------+-------------+-----------+------------------+-------------------+------------------+\n",
      "|station_area|       call_type|zipcode_of_incident|received_hour|day_of_week|             label|       distance_log|      duration_log|\n",
      "+------------+----------------+-------------------+-------------+-----------+------------------+-------------------+------------------+\n",
      "|          03|Medical Incident|              94109|           23|          3|              8.35|-0.5800741784344655|1.3652409519220583|\n",
      "|          37|Medical Incident|              94107|           23|          3|               5.8| -1.041458017321652| 0.597003320007043|\n",
      "|          37|Medical Incident|              94107|           23|          3|             10.25| -1.041458017321652| 0.597003320007043|\n",
      "|          41|Medical Incident|              94109|           23|          3|11.966666666666667|-0.8367940252822796|0.8685000680378065|\n",
      "|          41|  Structure Fire|              94109|           23|          3| 6.716666666666667|-0.3483108367617068|1.3987168811184478|\n",
      "+------------+----------------+-------------------+-------------+-----------+------------------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_big.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one or the other\n",
    "d = data_big\n",
    "cat_cols=[\"call_type\", \"zipcode_of_incident\", \"station_area\", \"received_hour\", \"day_of_week\"]\n",
    "hot_cols=[\"call_type\", \"zipcode_of_incident\", \"station_area\", \"received_hour\", \"day_of_week\"]\n",
    "\n",
    "# d = data_small\n",
    "# cat_cols=[]\n",
    "# hot_cols=[]\n",
    "\n",
    "processed_data = process(d,\n",
    "                         cat_cols=cat_cols,\n",
    "                         hot_cols=hot_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take approximately top ~10% of rows convert them to dataframe \n",
    "# Also provide the schema also to avoid errors\n",
    "test = ss.createDataFrame(processed_data.head(100000), processed_data.schema)\n",
    "\n",
    "#Take the rest of the rows\n",
    "train = processed_data.subtract(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.count(), test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pipelines\n",
    "va = VectorAssembler(outputCol=\"features\",\n",
    "                     inputCols=[x for x in processed_data.columns if x != 'label'])\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "dt = DecisionTreeRegressor()\n",
    "lr = LinearRegression()\n",
    "lr_ElasticNet = LinearRegression(maxIter=10, regParam=10, elasticNetParam=0.8)\n",
    "\n",
    "pipeline_lr = Pipeline(stages=[va,lr])\n",
    "pipeline_lr_ElasticNet = Pipeline(stages=[va,lr_ElasticNet])\n",
    "pipeline_dt = Pipeline(stages=[va,dt])\n",
    "pipeline_rf = Pipeline(stages=[va,rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "498.0646800994873"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit models\n",
    "start = timeit.default_timer()\n",
    "lr_fitted = pipeline_lr.fit(train)\n",
    "end = timeit.default_timer()\n",
    "end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "476.43625712394714"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "lr_e_fitted = pipeline_lr_ElasticNet.fit(train)\n",
    "end = timeit.default_timer()\n",
    "end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131.63317203521729"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "dt_fitted = pipeline_dt.fit(train)\n",
    "end = timeit.default_timer()\n",
    "end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213.0709388256073"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "rf_fitted = pipeline_rf.fit(train)\n",
    "end = timeit.default_timer()\n",
    "end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results - careful about re-running cells\n",
    "In each section, first cell is results on all columns and second cell uses just `distance` and `duration`. We see that the metrics are consistently better when we have use all of the columns and not just these two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model results\n",
    "def evaluate_regression(fitted_model, test_set):\n",
    "    test_pred = fitted_model.transform(test_set)\n",
    "    r2_ev = RegressionEvaluator(metricName='r2')\n",
    "    rmse_ev = RegressionEvaluator(metricName='rmse')\n",
    "    mae_ev = RegressionEvaluator(metricName='mae')\n",
    "    r2 = r2_ev.evaluate(test_pred)\n",
    "    rmse = rmse_ev.evaluate(test_pred)\n",
    "    mae = mae_ev.evaluate(test_pred)\n",
    "    return (r2,rmse,mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree:\n",
      "R^2:  0.0245728649704\n",
      "RMSE: 4.85811019003\n",
      "MAE: 3.34637837615\n"
     ]
    }
   ],
   "source": [
    "ev_dt = evaluate_regression(fitted_model=dt_fitted, test_set=test)\n",
    "print(\"Decision Tree:\\nR^2:  \" + str(ev_dt[0]) + \"\\nRMSE: \" + str(ev_dt[1]) + \"\\nMAE: \" + str(ev_dt[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression:\n",
      "R^2:  0.0382449456338\n",
      "RMSE: 4.82394317574\n",
      "MAE: 3.3039737344\n"
     ]
    }
   ],
   "source": [
    "ev_lr = evaluate_regression(fitted_model=lr_fitted, test_set=test)\n",
    "print(\"Linear Regression:\\nR^2:  \" + str(ev_lr[0]) + \"\\nRMSE: \" + str(ev_lr[1]) + \"\\nMAE: \" + str(ev_lr[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression w/ Elastic Net:\n",
      "R^2:  -0.0368547038269\n",
      "RMSE: 5.00874469868\n",
      "MAE: 3.45915225661\n"
     ]
    }
   ],
   "source": [
    "ev_lr_e = evaluate_regression(fitted_model=lr_e_fitted, test_set=test)\n",
    "print(\"Linear Regression w/ Elastic Net:\\nR^2:  \" + str(ev_lr_e[0]) + \"\\nRMSE: \" + str(ev_lr_e[1]) + \"\\nMAE: \" + str(ev_lr_e[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "R^2:  0.0229994180441\n",
      "RMSE: 4.86202688363\n",
      "MAE: 3.34406035385\n"
     ]
    }
   ],
   "source": [
    "ev_rf = evaluate_regression(fitted_model=rf_fitted, test_set=test)\n",
    "print(\"Random Forest:\\nR^2:  \" + str(ev_rf[0]) + \"\\nRMSE: \" + str(ev_rf[1]) + \"\\nMAE: \" + str(ev_rf[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
