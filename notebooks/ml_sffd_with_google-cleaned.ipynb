{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import numpy as np\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "ss = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'msds697jonross.and.friends' # Add your bucket name\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(bucket_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_fire = 'sffd.csv' # select file\n",
    "obj_fire = bucket.Object(key=file_name_fire) # S3 uses key-value structure where key is your file name\n",
    "file_content_fire = obj_fire.get()[\"Body\"].read().decode(\"utf-8\") # Read the Body which is the contents of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_goo = 'google_data.csv' # select file\n",
    "obj_goo = bucket.Object(key=file_name_goo) # S3 uses key-value structure where key is your file name\n",
    "file_content_goo = obj_goo.get()[\"Body\"].read().decode(\"utf-8\") # Read the Body which is the contents of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4557045"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of rows (subract header and empty line at end)\n",
    "rows_fire = file_content_fire.split('\\n')\n",
    "len(rows_fire)-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32074"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of rows (subract empty line at end)\n",
    "rows_goo = file_content_goo.split('\\n')\n",
    "len(rows_goo)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of  columns\n",
    "column_names_fire = rows_fire[0].split(',')\n",
    "n_cols = sc.broadcast(len(column_names_fire))\n",
    "n_cols.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call_type\n",
      "received_timestamp\n",
      "entry_timestamp\n",
      "dispatch_timestamp\n",
      "response_timestamp\n",
      "on_scene_timestamp\n",
      "transport_timestamp\n",
      "hospital_timestamp\n",
      "call_final_disposition\n",
      "available_timestamp\n",
      "address\n",
      "zipcode_of_incident\n",
      "battalion\n",
      "station_area\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(x for x in column_names_fire))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Medical Incident,2012-06-20 13:12:20+00:00,2012-06-20 13:12:53+00:00,2012-06-20 13:13:54+00:00,2012-06-20 13:14:19+00:00,2012-06-20 13:19:11+00:00,,2012-06-20 13:43:07+00:00,Code 2 Transport,2012-06-20 14:11:47+00:00,600 Block of CASTRO ST,94114,B06,06',\n",
       "       'Structure Fire,2010-01-04 11:23:56+00:00,2010-01-04 11:24:22+00:00,2010-01-04 11:24:47+00:00,2010-01-04 11:46:53+00:00,,,,Other,2010-01-04 12:01:03+00:00,3400 Block of 24TH ST,94110,B06,11'],\n",
       "      dtype='<U315')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly sample rows\n",
    "np.random.seed(1)\n",
    "sz=100000\n",
    "samples_fire = np.random.choice(rows_fire[1:], size=sz, replace=False)\n",
    "samples_fire[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = [f\"0{i}\" for i in range(1,10)] +\\\n",
    "    [str(i) for i in list(range(10,27))] +\\\n",
    "    ['28','29'] +\\\n",
    "    [str(i) for i in list(range(31,45))] +\\\n",
    "    ['48','49','51']\n",
    "stations = sc.broadcast(stations)\n",
    "# stations.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fire(x):\n",
    "    if len(x.split(',')) == n_cols.value:\n",
    "        return x.split(',')[13] in stations.value\n",
    "\n",
    "def map_fire(x):\n",
    "    return ((x.split(',')[13],x.split(',')[10]), x.split(',')[:2]+[x.split(',')[5]]+[x.split(',')[11]])\n",
    "\n",
    "def map_google(x):\n",
    "    s = x.split(',')\n",
    "    try:\n",
    "        if int(s[0]) < 10:\n",
    "            s[0] = '0' + s[0]\n",
    "    except:\n",
    "        pass\n",
    "    return (tuple(s[:2]), s[2:])\n",
    "\n",
    "def map_joined(x):\n",
    "    try:\n",
    "        return list(x[0])+x[1][0]+[float(y) for y in x[1][1]]\n",
    "    except:\n",
    "        return list(x[0])+x[1][0]+[None,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_fire = sc.parallelize(list(samples_fire))\\\n",
    "    .filter(filter_fire)\\\n",
    "    .map(map_fire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_google = sc.parallelize(rows_goo)\\\n",
    "    .map(map_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('06', '600 Block of CASTRO ST'),\n",
       "  ['Medical Incident',\n",
       "   '2012-06-20 13:12:20+00:00',\n",
       "   '2012-06-20 13:19:11+00:00',\n",
       "   '94114']),\n",
       " (('11', '3400 Block of 24TH ST'),\n",
       "  ['Structure Fire', '2010-01-04 11:23:56+00:00', '', '94110'])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_fire.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('01', '0 Block of 0NB OCTAVIA OF'), ['3724', '12.883333333333333']),\n",
       " (('01', '0 Block of 101 NB OCTAVIA OF'), ['4290', '9.416666666666666'])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_google.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = rdd_fire.leftOuterJoin(rdd_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('03', '500 Block of ELLIS ST'),\n",
       "  (['Structure Fire',\n",
       "    '2004-06-17 18:52:58+00:00',\n",
       "    '2004-06-17 19:00:36+00:00',\n",
       "    '94109'],\n",
       "   ['681', '2.816666666666667'])),\n",
       " (('03', '500 Block of ELLIS ST'),\n",
       "  (['Alarms',\n",
       "    '2011-03-04 04:01:43+00:00',\n",
       "    '2011-03-04 04:09:13+00:00',\n",
       "    '94109'],\n",
       "   ['681', '2.816666666666667']))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = joined.map(map_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['03',\n",
       "  '500 Block of ELLIS ST',\n",
       "  'Structure Fire',\n",
       "  '2004-06-17 18:52:58+00:00',\n",
       "  '2004-06-17 19:00:36+00:00',\n",
       "  '94109',\n",
       "  681.0,\n",
       "  2.816666666666667],\n",
       " ['03',\n",
       "  '500 Block of ELLIS ST',\n",
       "  'Alarms',\n",
       "  '2011-03-04 04:01:43+00:00',\n",
       "  '2011-03-04 04:09:13+00:00',\n",
       "  '94109',\n",
       "  681.0,\n",
       "  2.816666666666667]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1475"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of rows removed\n",
    "sz - rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"station_area\", StringType(), False),\n",
    "                     StructField(\"address\", StringType(), False),\n",
    "                     StructField(\"call_type\", StringType(), False),\n",
    "                     StructField(\"received_timestamp\", StringType(), False),\n",
    "                     StructField(\"on_scene_timestamp\", StringType(), False),\n",
    "                     StructField(\"zipcode_of_incident\", StringType(), False),\n",
    "                     StructField(\"distance\", DoubleType(), True),\n",
    "                     StructField(\"duration\", DoubleType(), True)\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ss.createDataFrame(rdd, schema) # .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_area: string (nullable = false)\n",
      " |-- address: string (nullable = false)\n",
      " |-- call_type: string (nullable = false)\n",
      " |-- received_timestamp: string (nullable = false)\n",
      " |-- on_scene_timestamp: string (nullable = false)\n",
      " |-- zipcode_of_incident: string (nullable = false)\n",
      " |-- distance: double (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to timestamps\n",
    "my_rows = ['received_timestamp',\n",
    "          'on_scene_timestamp']\n",
    "\n",
    "df_w_time = df\n",
    "for row in my_rows:\n",
    "    df_w_time = df_w_time.withColumn(row, to_timestamp(df[row], format = 'yyyy-MM-dd HH:mm:ss+00:00'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_time = df_w_time.withColumn(\"received_hour\",\n",
    "                                 hour(\"received_timestamp\")) \\\n",
    "            .withColumn(\"day_of_week\",\n",
    "                        date_format('received_timestamp', 'u').alias('dow_number'))\n",
    "# df_w_time.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create More Interesting DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_full = df_w_time.select('station_area',\n",
    "                 'address',\n",
    "                 'call_type',\n",
    "                 'received_timestamp',\n",
    "                 'on_scene_timestamp',\n",
    "                 'zipcode_of_incident',\n",
    "                 'distance',\n",
    "                 'duration',\n",
    "                 'received_hour',\n",
    "                 'day_of_week')\\\n",
    "    .withColumn(\"distance\", \n",
    "                col('distance') / 1609.34 )\\\n",
    "    .withColumn(\"label\", \n",
    "                (unix_timestamp('on_scene_timestamp') - unix_timestamp('received_timestamp')) / 60 )\\\n",
    "    .withColumn(\"speed\", \n",
    "                col('distance') / col('duration') * 60 )\\\n",
    "    .withColumn(\"label_ratio\", \n",
    "                col('label') / col('duration') )\\\n",
    "    .orderBy('received_timestamp', ascending=[0])\\\n",
    "    .na.drop(subset=[\"label\"])\\\n",
    "    .where('label >= 0')\\\n",
    "    .select('station_area',\n",
    "        'call_type',\n",
    "        'zipcode_of_incident',\n",
    "        'distance',\n",
    "        'duration',\n",
    "        'speed',\n",
    "        'label_ratio',\n",
    "        'received_hour',\n",
    "        'day_of_week',\n",
    "        'label')\n",
    "\n",
    "# data_1 is the largest dataset\n",
    "# has all columns and ony removes missing google data\n",
    "data_1 = data_full.na.drop(subset=[\"distance\", \"duration\"]) # (removes about 50 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_area: string (nullable = false)\n",
      " |-- call_type: string (nullable = false)\n",
      " |-- zipcode_of_incident: string (nullable = false)\n",
      " |-- distance: double (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- speed: double (nullable = true)\n",
      " |-- label_ratio: double (nullable = true)\n",
      " |-- received_hour: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `label`: response time\n",
    "- `duration`: google api estimate of driving time from station to address (in minutes)\n",
    "- `distance`: google api estimate of driving distance from station to address\n",
    "- `speed`: distance / duration * 60 (estimated average speed in mph)\n",
    "- `label_ratio`: label / duration. **Using this variable for eda**. The idea is to try to find the observations where distance and duration would underestimate most severely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.01553431841624517,\n",
       " 0.5710415449811724,\n",
       " 1.2334248822498666,\n",
       " 1.880895273838965,\n",
       " 2.403469745361453,\n",
       " 5.0772366311655714,\n",
       " 10.992083711335082]"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1.approxQuantile(\"distance\", [.0001,.001,.5,.9,.97,.98,.99,.999], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed 2063 rows\n"
     ]
    }
   ],
   "source": [
    "# removing rows where distance (in miles) is too large or too small\n",
    "data_2 = data_1.where('distance between .01 and 2') \n",
    "print(f'removed {data_1.count() - data_2.count()} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.1621735044753185,\n",
       " 6.018438792122414,\n",
       " 6.363339442895303,\n",
       " 6.53586920985618,\n",
       " 7.339965451675843,\n",
       " 9.760370774380068,\n",
       " 13.378074322312331,\n",
       " 15.084683683777293,\n",
       " 15.638797017982748,\n",
       " 16.75797735497192,\n",
       " 18.40977432170738,\n",
       " 21.29874549624261]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to see that speeds look reasonable (mph)\n",
    "data_2.approxQuantile(\"speed\", [.001,.01,.02,.03,.1,.5,.9,.96,.97,.98,.99,.999], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 1.9,\n",
       " 2.5166666666666666,\n",
       " 2.816666666666667,\n",
       " 3.816666666666667,\n",
       " 6.566666666666666,\n",
       " 14.566666666666666,\n",
       " 21.2,\n",
       " 23.5,\n",
       " 26.783333333333335,\n",
       " 33.35,\n",
       " 77.85]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2.approxQuantile(\"label\", [.001,.01,.02,.03,.1,.5,.9,.96,.97,.98,.99,.999], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed 1404 rows\n"
     ]
    }
   ],
   "source": [
    "# removing rows where response time (in minutes) is too large or too small\n",
    "data_3 = data_2.where('label between 1 and 30') \n",
    "print(f'removed {data_2.count() - data_3.count()} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2263083451202263,\n",
       " 0.3801801801801802,\n",
       " 0.8526315789473685,\n",
       " 2.0104166666666665,\n",
       " 5.546218487394958,\n",
       " 8.72340425531915,\n",
       " 10.022727272727273,\n",
       " 12.333333333333334,\n",
       " 17.705882352941178,\n",
       " 68.00000000000001]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_3.approxQuantile(\"label_ratio\", [.001,.01,.1,.5,.9,.96,.97,.98,.99,.999], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed 595 rows\n"
     ]
    }
   ],
   "source": [
    "# removing rows where response time is too much larger than duration\n",
    "data_4 = data_3.where('label_ratio < 20') \n",
    "print(f'removed {data_3.count() - data_4.count()} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+-------------------+\n",
      "|station_area|        call_type|zipcode_of_incident|\n",
      "+------------+-----------------+-------------------+\n",
      "|          11|           Alarms|              94110|\n",
      "|          31| Medical Incident|              94118|\n",
      "|          11|           Alarms|              94131|\n",
      "|          07|   Structure Fire|              94110|\n",
      "|          07| Medical Incident|              94110|\n",
      "|          29|Traffic Collision|              94103|\n",
      "|          36| Medical Incident|              94102|\n",
      "|          05|Traffic Collision|              94102|\n",
      "|          36| Medical Incident|              94103|\n",
      "|          06| Medical Incident|              94114|\n",
      "|          36| Medical Incident|              94103|\n",
      "|          01|           Alarms|              94103|\n",
      "|          36| Medical Incident|              94102|\n",
      "|          17| Medical Incident|              94124|\n",
      "|          31|           Alarms|              94121|\n",
      "|          05| Medical Incident|              94102|\n",
      "|          03| Medical Incident|              94109|\n",
      "|          35|           Alarms|              94107|\n",
      "|          35| Medical Incident|              94105|\n",
      "|          03| Medical Incident|              94102|\n",
      "+------------+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------------+------------------+-------------+-----------+------------------+\n",
      "|           distance|          duration|received_hour|day_of_week|             label|\n",
      "+-------------------+------------------+-------------+-----------+------------------+\n",
      "|  0.620751363913157| 3.183333333333333|           23|          3| 5.633333333333334|\n",
      "| 0.5934109635005654|              3.65|           22|          3| 5.083333333333333|\n",
      "|0.19511103930803933|1.0833333333333333|           22|          3| 8.483333333333333|\n",
      "| 0.6170231274932582| 3.466666666666667|           21|          3| 4.233333333333333|\n",
      "| 0.6306933276995539|3.8333333333333335|           20|          3|              13.1|\n",
      "| 0.4399318975480632| 2.466666666666667|           19|          3|              5.25|\n",
      "|0.25289870381647134| 2.033333333333333|           19|          3|               8.1|\n",
      "|  1.913828028881405|             12.25|           18|          3|3.2666666666666666|\n",
      "|0.47162190711720337|              3.75|           18|          3|10.883333333333333|\n",
      "|  0.733219829246772| 4.633333333333334|           18|          3|              22.0|\n",
      "| 0.4231548336585184| 4.216666666666667|           13|          3| 5.316666666666666|\n",
      "|  1.312339219804392|              10.4|           11|          3| 5.716666666666667|\n",
      "| 1.0855381709272125| 5.966666666666667|           11|          3|              8.05|\n",
      "| 0.8953981135123715| 4.233333333333333|            9|          3| 4.033333333333333|\n",
      "| 0.7729876843923597|3.2333333333333334|            9|          3|               8.3|\n",
      "| 1.8423701641666772|11.616666666666667|            9|          3|              5.75|\n",
      "| 0.4368250338648142|              2.95|            6|          3| 7.183333333333334|\n",
      "| 0.7176855108305268|3.8666666666666667|            4|          3| 9.316666666666666|\n",
      "| 0.4461456249145613|              2.75|            2|          3|16.716666666666665|\n",
      "|   0.54680800825183| 3.466666666666667|            0|          3|21.766666666666666|\n",
      "+-------------------+------------------+-------------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_cleaned = data_4\\\n",
    "    .select('station_area','call_type','zipcode_of_incident',\n",
    "            'distance','duration','received_hour','day_of_week','label')\n",
    "data_cleaned.select('station_area','call_type','zipcode_of_incident').show()\n",
    "data_cleaned.select('distance','duration','received_hour','day_of_week','label').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_strings(df):\n",
    "    return df.replace('','unknown')\n",
    "# dfnonas = remove_empty_strings(small_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting strings to numeric values\n",
    "def indexStringColumns(df, cols):\n",
    "    #variable newdf will be updated several times\n",
    "    newdf = df\n",
    "    \n",
    "    for c in cols:\n",
    "        #For each given colum, fits StringIndexerModel.\n",
    "        si = StringIndexer(inputCol=c, outputCol=c+\"-num\")\n",
    "        sm = si.fit(newdf)\n",
    "        #Creates a DataFame by putting the transformed values in the new colum with suffix \"-num\" \n",
    "        #and then drops the original columns.\n",
    "        #and drop the \"-num\" suffix. \n",
    "        newdf = sm.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-num\", c)\n",
    "    return newdf\n",
    "\n",
    "# dfnumeric = indexStringColumns(dfnonas, [\"call_type\", \"zipcode_of_incident\", \"station_area\"])\n",
    "# dfnumeric = indexStringColumns(dfnonas, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncodeColumns(df, cols):\n",
    "    newdf = df\n",
    "    for c in cols:\n",
    "        #For each given colum, create OneHotEncoder. \n",
    "        #dropLast : Whether to drop the last category in the encoded vector (default: true)\n",
    "        onehotenc = OneHotEncoder(inputCol=c, outputCol=c+\"-onehot\", dropLast=False)\n",
    "        #Creates a DataFame by putting the transformed values in the new colum with suffix \"-onehot\" \n",
    "        #and then drops the original columns.\n",
    "        #and drop the \"-onehot\" suffix. \n",
    "        newdf = onehotenc.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-onehot\", c)\n",
    "    return newdf\n",
    "\n",
    "# dfhot = oneHotEncodeColumns(dfnumeric, [\"call_type\", \"zipcode_of_incident\", \"station_area\"])\n",
    "# dfhot = oneHotEncodeColumns(dfnumeric, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df, cat_cols, hot_cols):\n",
    "    dfnonas = remove_empty_strings(df)\n",
    "    dfnumeric = indexStringColumns(dfnonas, cat_cols)\n",
    "    dfhot = oneHotEncodeColumns(dfnumeric, hot_cols)\n",
    "    return dfhot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one or the other\n",
    "processed_data = process(data_cleaned.select('distance','duration','label'),\n",
    "                         cat_cols=[],\n",
    "                         hot_cols=[])\n",
    "\n",
    "# processed_data = process(data_cleaned,\n",
    "#                          cat_cols=[\"call_type\", \"zipcode_of_incident\", \"station_area\", \"received_hour\", \"day_of_week\"],\n",
    "#                          hot_cols=[\"call_type\", \"zipcode_of_incident\", \"station_area\", \"received_hour\", \"day_of_week\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take approximately top ~10% of rows convert them to dataframe \n",
    "# Also provide the schema also to avoid errors\n",
    "test = ss.createDataFrame(processed_data.head(8000), processed_data.schema)\n",
    "\n",
    "#Take the rest of the rows\n",
    "train = processed_data.subtract(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61029 8000\n"
     ]
    }
   ],
   "source": [
    "print(train.count(), test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pipelines\n",
    "va = VectorAssembler(outputCol=\"features\",\n",
    "                     inputCols=[x for x in processed_data.columns if x != 'label'])\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "dt = DecisionTreeRegressor()\n",
    "lr = LinearRegression()\n",
    "lr_ElasticNet = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "pipeline_lr = Pipeline(stages=[va,lr])\n",
    "pipeline_lr_ElasticNet = Pipeline(stages=[va,lr_ElasticNet])\n",
    "pipeline_dt = Pipeline(stages=[va,dt])\n",
    "pipeline_rf = Pipeline(stages=[va,rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit models\n",
    "lr_fitted = pipeline_lr.fit(train)\n",
    "lr_e_fitted = pipeline_lr_ElasticNet.fit(train)\n",
    "dt_fitted = pipeline_dt.fit(train)\n",
    "rf_fitted = pipeline_rf.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results - careful about re-running cells\n",
    "In each section, first cell is results on all columns and second cell uses just `distance` and `duration`. We see that the metrics are consistently better when we have use all of the columns and not just these two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model results\n",
    "def evaluate_regression(fitted_model, test_set):\n",
    "    test_pred = fitted_model.transform(test_set)\n",
    "    r2_ev = RegressionEvaluator(metricName='r2')\n",
    "    rmse_ev = RegressionEvaluator(metricName='rmse')\n",
    "    mae_ev = RegressionEvaluator(metricName='mae')\n",
    "    r2 = r2_ev.evaluate(test_pred)\n",
    "    rmse = rmse_ev.evaluate(test_pred)\n",
    "    mae = mae_ev.evaluate(test_pred)\n",
    "    return (r2,rmse,mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression:\n",
      "R^2:  -0.022815405417011325\n",
      "RMSE: 4.96772438969419\n",
      "MAE:  3.4244997440878224\n",
      "\n",
      "\n",
      "Elastic Net Regression:\n",
      "R^2:  -0.02608933445342676\n",
      "RMSE: 4.975668630164337\n",
      "MAE:  3.4403794737203923\n",
      "\n",
      "\n",
      "Decision Tree:\n",
      "R^2:  -0.01782338654641391\n",
      "RMSE: 4.955586663423365\n",
      "MAE:  3.4258079945645683\n",
      "\n",
      "\n",
      "Random Forest\n",
      "R^2:  -0.01684640014176164\n",
      "RMSE: 4.953207712791327\n",
      "MAE:  3.4238245095136017\n"
     ]
    }
   ],
   "source": [
    "# just distance and duration\n",
    "ev_lr = evaluate_regression(fitted_model=lr_fitted, test_set=test)\n",
    "print(f\"Linear Regression:\\nR^2:  {ev_lr[0]}\\nRMSE: {ev_lr[1]}\\nMAE:  {ev_lr[2]}\")\n",
    "print('\\n')\n",
    "ev_e = evaluate_regression(fitted_model=lr_e_fitted, test_set=test)\n",
    "print(f\"Elastic Net Regression:\\nR^2:  {ev_e[0]}\\nRMSE: {ev_e[1]}\\nMAE:  {ev_e[2]}\")\n",
    "print('\\n')\n",
    "ev_dt = evaluate_regression(fitted_model=dt_fitted, test_set=test)\n",
    "print(f\"Decision Tree:\\nR^2:  {ev_dt[0]}\\nRMSE: {ev_dt[1]}\\nMAE:  {ev_dt[2]}\")\n",
    "print('\\n')\n",
    "ev_rf = evaluate_regression(fitted_model=rf_fitted, test_set=test)\n",
    "print(f\"Random Forest\\nR^2:  {ev_rf[0]}\\nRMSE: {ev_rf[1]}\\nMAE:  {ev_rf[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression:\n",
      "R^2:  0.040529356313042886\n",
      "RMSE: 4.8114359463015095\n",
      "MAE:  3.296128921495305\n",
      "\n",
      "\n",
      "Elastic Net Regression:\n",
      "R^2:  0.0056151653693420345\n",
      "RMSE: 4.898195431392101\n",
      "MAE:  3.3539429031301595\n",
      "\n",
      "\n",
      "Decision Tree:\n",
      "R^2:  0.03007135145173201\n",
      "RMSE: 4.837586641684353\n",
      "MAE:  3.3317944451983403\n",
      "\n",
      "\n",
      "Random Forest\n",
      "R^2:  0.027825524400997392\n",
      "RMSE: 4.843184012803788\n",
      "MAE:  3.3224205420135933\n"
     ]
    }
   ],
   "source": [
    "# all predictors\n",
    "ev_lr = evaluate_regression(fitted_model=lr_fitted, test_set=test)\n",
    "print(f\"Linear Regression:\\nR^2:  {ev_lr[0]}\\nRMSE: {ev_lr[1]}\\nMAE:  {ev_lr[2]}\")\n",
    "print('\\n')\n",
    "ev_e = evaluate_regression(fitted_model=lr_e_fitted, test_set=test)\n",
    "print(f\"Elastic Net Regression:\\nR^2:  {ev_e[0]}\\nRMSE: {ev_e[1]}\\nMAE:  {ev_e[2]}\")\n",
    "print('\\n')\n",
    "ev_dt = evaluate_regression(fitted_model=dt_fitted, test_set=test)\n",
    "print(f\"Decision Tree:\\nR^2:  {ev_dt[0]}\\nRMSE: {ev_dt[1]}\\nMAE:  {ev_dt[2]}\")\n",
    "print('\\n')\n",
    "ev_rf = evaluate_regression(fitted_model=rf_fitted, test_set=test)\n",
    "print(f\"Random Forest\\nR^2:  {ev_rf[0]}\\nRMSE: {ev_rf[1]}\\nMAE:  {ev_rf[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there is quite a bit of overfitting going on here. That isn't too surprising since there wasn't too much preprocessing done other than outlier removal. Some of the categorical features have a lot of categories but our target variable may not change much across those categories which could also be a problem. Hyper-parameters weren't chosen very carefully. Right now, regularized regression doesn't really help the linear model and ensembling doesn't really help the decision tree, but maybe better hyper-parameters would change that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
